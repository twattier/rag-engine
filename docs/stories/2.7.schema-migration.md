# Story 2.7: Implement Metadata Schema Migration and Reindexing

**Epic:** Epic 2 - Multi-Format Document Ingestion Pipeline
**Story ID:** 2.7
**Status:** Draft
**Estimated Effort:** 5 story points (6-8 hours)

---

## User Story

**As a** knowledge base administrator,
**I want** to update my metadata schema and reindex existing documents,
**so that** I can adapt the knowledge base to evolving organizational needs without redeployment.

---

## Acceptance Criteria

1. API endpoint `PUT /api/v1/config/metadata-schema` accepts updated metadata schema YAML/JSON and persists to `config/metadata-schema.yaml`
2. Schema validation ensures backward compatibility:
   - New fields must be optional or have defaults
   - Existing required fields cannot be removed
   - Field type changes rejected (breaking change)
3. Schema update triggers reindexing workflow:
   - Option 1 (immediate): Automatically reindex all documents in background
   - Option 2 (deferred): Mark schema as "pending reindex," user triggers manually
4. API endpoint `POST /api/v1/documents/reindex` triggers reindexing:
   - Accepts optional filters (document IDs, date ranges, metadata criteria)
   - Returns reindex_job_id for progress tracking
   - Reindexing updates document metadata fields without re-parsing content
5. API endpoint `GET /api/v1/documents/reindex/{job_id}/status` returns:
   - total_documents, processed_count, failed_count, status ("in_progress", "completed", "failed")
   - Estimated time remaining
6. Existing documents with missing new optional fields use schema defaults
7. Schema changes logged with timestamp, user, and change description to audit log
8. Documentation in `docs/schema-migration.md` explains:
   - When to use metadata schema updates
   - Backward compatibility requirements
   - Reindexing workflow and performance considerations
   - Examples of common schema evolutions

---

## Tasks / Subtasks

- [ ] **Task 1: Create schema update endpoint** (AC: 1)
  - [ ] Add `PUT /api/v1/config/metadata-schema` to config router
  - [ ] Accept metadata schema as JSON or YAML
  - [ ] Parse and validate schema format
  - [ ] Persist to `config/metadata-schema.yaml` file
  - [ ] Reload schema cache
  - [ ] Return updated schema confirmation

- [ ] **Task 2: Implement backward compatibility validation** (AC: 2)
  - [ ] Create `shared/services/schema_validator.py`
  - [ ] Implement `validate_schema_compatibility(old_schema, new_schema)` function
  - [ ] Check: new fields are optional or have defaults
  - [ ] Check: existing required fields not removed
  - [ ] Check: field types unchanged (no type mutations)
  - [ ] Return validation errors with specific incompatibility reasons

- [ ] **Task 3: Implement schema change detection** (AC: 3)
  - [ ] Compare old and new schemas to detect changes
  - [ ] Identify added fields, removed fields, type changes
  - [ ] Determine if reindexing is required
  - [ ] Flag schema as "pending reindex" if changes detected

- [ ] **Task 4: Create reindexing service** (AC: 4, 5, 6)
  - [ ] Create `services/api/services/reindex_service.py`
  - [ ] Implement `async def start_reindex(filters)` method
  - [ ] Generate unique reindex_job_id (UUID)
  - [ ] Query documents matching filters from Neo4j
  - [ ] Process documents in background task
  - [ ] Apply new schema defaults to existing documents
  - [ ] Update document metadata in Neo4j without re-parsing

- [ ] **Task 5: Create reindex endpoint** (AC: 4)
  - [ ] Add `POST /api/v1/documents/reindex` to documents router
  - [ ] Accept optional filters (document IDs, date ranges, metadata criteria)
  - [ ] Validate filters
  - [ ] Start reindexing background task
  - [ ] Return 202 Accepted with reindex_job_id

- [ ] **Task 6: Implement reindex status tracking** (AC: 5)
  - [ ] Create `ReindexStatus` Pydantic model
  - [ ] Track: job_id, total_documents, processed_count, failed_count, status
  - [ ] Store reindex status in in-memory dictionary (MVP)
  - [ ] Update status in real-time during reindexing
  - [ ] Calculate estimated time remaining based on progress

- [ ] **Task 7: Create reindex status endpoint** (AC: 5)
  - [ ] Add `GET /api/v1/documents/reindex/{job_id}/status` to documents router
  - [ ] Return ReindexStatusResponse with all tracking fields
  - [ ] Return 404 if job_id not found
  - [ ] Include estimated time remaining
  - [ ] Include list of failed documents with errors

- [ ] **Task 8: Implement schema change audit logging** (AC: 7)
  - [ ] Create `shared/models/audit_log.py`
  - [ ] Define `SchemaChangeLog` Pydantic model
  - [ ] Log schema changes to structured logs
  - [ ] Include: timestamp, user (API key), old schema, new schema, change description
  - [ ] Optionally persist to Neo4j or file for audit trail

- [ ] **Task 9: Create schema migration documentation** (AC: 8)
  - [ ] Create `docs/schema-migration.md`
  - [ ] Document when to use metadata schema updates
  - [ ] Explain backward compatibility requirements with examples
  - [ ] Document reindexing workflow (immediate vs. deferred)
  - [ ] Provide performance considerations and best practices
  - [ ] Include common schema evolution examples
  - [ ] Document troubleshooting steps

- [ ] **Task 10: Create integration tests** (AC: 1, 2, 3, 4, 5, 6)
  - [ ] Create `services/api/tests/integration/test_schema_migration.py`
  - [ ] Test schema update with backward compatible changes
  - [ ] Test schema update rejection for breaking changes
  - [ ] Test reindexing with default field values
  - [ ] Test reindex status tracking
  - [ ] Test reindex with filters (document IDs, date range)
  - [ ] Test reindex idempotency (running multiple times)

---

## Dev Notes

### Tech Stack
[Source: architecture/tech-stack.md]

- **Python**: 3.11+ (async/await, background tasks)
- **Backend Framework**: FastAPI 0.115+ (API endpoints)
- **Validation**: Pydantic 2.x (schema models)
- **YAML Parser**: PyYAML (schema persistence)
- **Graph Database**: Neo4j 5.x (document updates)
- **Logging**: structlog (audit logging)
- **Testing Framework**: pytest with pytest-asyncio

### Project Structure
[Source: architecture/unified-project-structure.md]

File locations for implementation:

```
services/api/
├── routers/
│   ├── config.py                # Schema update endpoint
│   └── documents.py             # Reindex endpoints
├── services/
│   └── reindex_service.py       # Reindexing logic
├── models/
│   ├── requests.py              # ReindexRequest
│   └── responses.py             # ReindexStatusResponse
└── tests/
    └── integration/
        └── test_schema_migration.py

shared/
├── services/
│   ├── __init__.py
│   └── schema_validator.py      # Backward compatibility validation
├── models/
│   └── audit_log.py             # Schema change audit log
└── database/
    └── document_repository.py   # Update metadata methods

docs/
└── schema-migration.md          # User documentation
```

### Component Architecture
[Source: architecture/components.md]

**Schema Migration Responsibility:**
Enable dynamic metadata schema updates with backward compatibility validation and document reindexing to maintain data consistency.

**Key Interfaces to Implement:**
- `PUT /api/v1/config/metadata-schema` - Update metadata schema
- `POST /api/v1/documents/reindex` - Trigger reindexing
- `GET /api/v1/documents/reindex/{job_id}/status` - Track reindex progress
- `validate_schema_compatibility(old, new)` - Validate backward compatibility
- `async def reindex_documents(filters, new_schema)` - Apply schema changes

**Dependencies:**
- Metadata Schema (Story 2.2)
- Document Management (Story 2.6)
- Neo4j Database

### API Endpoint Specification
[Source: Epic 2 Story 2.7 AC]

**Endpoint:** `PUT /api/v1/config/metadata-schema`

**Request Format (JSON):**
```json
{
  "metadata_fields": [
    {
      "field_name": "author",
      "type": "string",
      "required": false,
      "default": "Unknown",
      "description": "Document author"
    },
    {
      "field_name": "priority",
      "type": "integer",
      "required": false,
      "default": 3,
      "description": "Document priority (1-5)"
    }
  ]
}
```

**Request Format (YAML):**
```yaml
metadata_fields:
  - field_name: author
    type: string
    required: false
    default: "Unknown"
    description: "Document author"

  - field_name: priority
    type: integer
    required: false
    default: 3
    description: "Document priority (1-5)"
```

**Response Format (200 OK - Compatible):**
```json
{
  "message": "Metadata schema updated successfully",
  "schema_version": "2.0",
  "changes_detected": true,
  "reindex_required": true,
  "reindex_status": "pending",
  "added_fields": ["priority"],
  "removed_fields": [],
  "modified_fields": []
}
```

**Error Response (400 Bad Request - Incompatible):**
```json
{
  "error": {
    "code": "SCHEMA_INCOMPATIBLE",
    "message": "Schema update rejected: Breaking changes detected",
    "incompatibilities": [
      {
        "field": "author",
        "issue": "Cannot change required field to optional"
      },
      {
        "field": "date_created",
        "issue": "Cannot change field type from 'date' to 'string'"
      },
      {
        "field": "department",
        "issue": "Cannot remove required field"
      }
    ]
  }
}
```

**Endpoint:** `POST /api/v1/documents/reindex`

**Request Format:**
```json
{
  "filters": {
    "document_ids": ["550e8400-e29b-41d4-a716-446655440000"],
    "ingestion_date_from": "2025-10-01",
    "ingestion_date_to": "2025-10-16",
    "status": "indexed",
    "metadata": {
      "department": "engineering"
    }
  }
}
```

**Response Format (202 Accepted):**
```json
{
  "reindex_job_id": "7c9e6679-7425-40de-944b-e07fc1f90ae7",
  "total_documents": 150,
  "status": "in_progress",
  "message": "Reindexing started. Use reindex_job_id to check status"
}
```

**Endpoint:** `GET /api/v1/documents/reindex/{job_id}/status`

**Response Format (200 OK - In Progress):**
```json
{
  "reindex_job_id": "7c9e6679-7425-40de-944b-e07fc1f90ae7",
  "total_documents": 150,
  "processed_count": 75,
  "failed_count": 2,
  "status": "in_progress",
  "estimated_completion_time": "2025-10-16T14:45:00Z",
  "failed_documents": [
    {
      "document_id": "660f9511-f3ac-52e5-b827-557766551111",
      "error": "Failed to apply new metadata field: Invalid default value"
    }
  ]
}
```

**Response Format (200 OK - Completed):**
```json
{
  "reindex_job_id": "7c9e6679-7425-40de-944b-e07fc1f90ae7",
  "total_documents": 150,
  "processed_count": 148,
  "failed_count": 2,
  "status": "completed",
  "completed_at": "2025-10-16T14:45:00Z",
  "processing_time_seconds": 300,
  "failed_documents": [
    {
      "document_id": "660f9511-f3ac-52e5-b827-557766551111",
      "error": "Failed to apply new metadata field: Invalid default value"
    }
  ]
}
```

### Backward Compatibility Validation
[Source: Epic 2 Story 2.7 AC2]

**Validation Rules:**

1. **New Fields**: Must be optional or have default values
   - Valid: Adding optional field without default
   - Valid: Adding optional field with default value
   - Invalid: Adding required field without default

2. **Existing Required Fields**: Cannot be removed
   - Invalid: Removing required field
   - Valid: Removing optional field

3. **Field Type Changes**: Not allowed (breaking change)
   - Invalid: Changing `string` to `integer`
   - Invalid: Changing `date` to `string`
   - Valid: No type change

**Implementation:**
```python
# shared/services/schema_validator.py
from typing import List, Dict
from shared.models.metadata import MetadataSchema

class SchemaIncompatibility:
    """Represents a schema incompatibility."""
    def __init__(self, field: str, issue: str):
        self.field = field
        self.issue = issue

def validate_schema_compatibility(
    old_schema: MetadataSchema,
    new_schema: MetadataSchema
) -> List[SchemaIncompatibility]:
    """
    Validate backward compatibility between schemas.
    Returns list of incompatibilities (empty if compatible).
    """
    incompatibilities = []

    old_fields = {f.field_name: f for f in old_schema.metadata_fields}
    new_fields = {f.field_name: f for f in new_schema.metadata_fields}

    # Check for removed required fields
    for field_name, field_def in old_fields.items():
        if field_name not in new_fields and field_def.required:
            incompatibilities.append(
                SchemaIncompatibility(
                    field=field_name,
                    issue="Cannot remove required field"
                )
            )

    # Check for field type changes
    for field_name, new_field in new_fields.items():
        if field_name in old_fields:
            old_field = old_fields[field_name]
            if old_field.type != new_field.type:
                incompatibilities.append(
                    SchemaIncompatibility(
                        field=field_name,
                        issue=f"Cannot change field type from '{old_field.type}' to '{new_field.type}'"
                    )
                )

    # Check for new required fields without defaults
    for field_name, new_field in new_fields.items():
        if field_name not in old_fields and new_field.required and new_field.default is None:
            incompatibilities.append(
                SchemaIncompatibility(
                    field=field_name,
                    issue="Cannot add required field without default value"
                )
            )

    return incompatibilities
```

### Reindexing Implementation
[Source: Epic 2 Story 2.7 AC4, AC6]

**Reindexing Process:**
1. Query documents matching filters from Neo4j
2. For each document:
   - Load current metadata
   - Apply new schema (add missing fields with defaults)
   - Validate metadata against new schema
   - Update document metadata in Neo4j
   - Track progress and errors
3. Update reindex job status

**Implementation:**
```python
# services/api/services/reindex_service.py
from typing import Dict, List, Optional
import time
from uuid import UUID, uuid4
from pydantic import BaseModel
from shared.models.metadata import MetadataSchema
from shared.database.document_repository import DocumentRepository

class ReindexStatus(BaseModel):
    """Reindex job status."""
    reindex_job_id: UUID
    total_documents: int
    processed_count: int
    failed_count: int
    status: str  # "in_progress", "completed", "failed"
    failed_documents: List[Dict[str, str]]
    processing_time_seconds: float = 0
    estimated_completion_time: Optional[str] = None

class ReindexService:
    """Service for reindexing documents with new schema."""

    def __init__(self, document_repository: DocumentRepository):
        self.repository = document_repository
        self.jobs: Dict[UUID, ReindexStatus] = {}

    async def start_reindex(
        self,
        filters: Dict[str, any],
        new_schema: MetadataSchema
    ) -> UUID:
        """Start reindexing job."""
        job_id = uuid4()

        # Query documents matching filters
        documents = await self.repository.list_documents(filters, limit=10000, offset=0)
        total_docs = len(documents)

        # Initialize job status
        job_status = ReindexStatus(
            reindex_job_id=job_id,
            total_documents=total_docs,
            processed_count=0,
            failed_count=0,
            status="in_progress",
            failed_documents=[]
        )
        self.jobs[job_id] = job_status

        # Start background processing
        asyncio.create_task(
            self._process_reindex(job_id, documents, new_schema)
        )

        return job_id

    async def _process_reindex(
        self,
        job_id: UUID,
        documents: List[Dict],
        new_schema: MetadataSchema
    ):
        """Process reindexing in background."""
        job_status = self.jobs[job_id]
        start_time = time.time()

        for doc in documents:
            try:
                # Load current metadata
                metadata = doc["metadata"]

                # Apply new schema defaults
                for field_def in new_schema.metadata_fields:
                    if field_def.field_name not in metadata and field_def.default is not None:
                        metadata[field_def.field_name] = field_def.default

                # Validate metadata against new schema
                new_schema.validate_metadata(metadata)

                # Update document in Neo4j
                await self.repository.update_document_metadata(
                    document_id=doc["document_id"],
                    metadata=metadata
                )

                job_status.processed_count += 1

            except Exception as e:
                logger.error(
                    "Reindex document failed",
                    job_id=str(job_id),
                    document_id=doc["document_id"],
                    error=str(e)
                )

                job_status.failed_count += 1
                job_status.failed_documents.append({
                    "document_id": doc["document_id"],
                    "error": str(e)
                })

        # Update final status
        if job_status.failed_count == 0:
            job_status.status = "completed"
        elif job_status.processed_count > 0:
            job_status.status = "completed"  # Partial success
        else:
            job_status.status = "failed"

        job_status.processing_time_seconds = time.time() - start_time

        logger.info(
            "Reindex job completed",
            job_id=str(job_id),
            total=job_status.total_documents,
            processed=job_status.processed_count,
            failed=job_status.failed_count,
            duration_seconds=job_status.processing_time_seconds
        )

    def get_reindex_status(self, job_id: UUID) -> ReindexStatus:
        """Get reindex job status."""
        if job_id not in self.jobs:
            raise ValueError(f"Reindex job {job_id} not found")
        return self.jobs[job_id]
```

### Schema Change Audit Logging
[Source: Epic 2 Story 2.7 AC7]

```python
# shared/models/audit_log.py
from datetime import datetime
from pydantic import BaseModel
from typing import Dict, List

class SchemaChangeLog(BaseModel):
    """Audit log for schema changes."""
    timestamp: datetime
    user: str  # API key or user identifier
    old_schema_version: str
    new_schema_version: str
    change_description: str
    added_fields: List[str]
    removed_fields: List[str]
    modified_fields: List[str]

def log_schema_change(
    old_schema: MetadataSchema,
    new_schema: MetadataSchema,
    user: str
):
    """Log schema change to structured logs."""
    old_fields = {f.field_name for f in old_schema.metadata_fields}
    new_fields = {f.field_name for f in new_schema.metadata_fields}

    added = list(new_fields - old_fields)
    removed = list(old_fields - new_fields)

    change_log = SchemaChangeLog(
        timestamp=datetime.utcnow(),
        user=user,
        old_schema_version="1.0",  # Track versions
        new_schema_version="2.0",
        change_description=f"Added {len(added)} fields, removed {len(removed)} fields",
        added_fields=added,
        removed_fields=removed,
        modified_fields=[]
    )

    logger.info(
        "Metadata schema updated",
        audit_log=change_log.model_dump()
    )
```

### Coding Standards
[Source: architecture/coding-standards.md]

**Critical Rules:**
- **Type Safety:** Use Pydantic V2 for all schema models
- **Backward Compatibility:** Always validate before applying schema changes
- **Error Handling:** Return clear incompatibility reasons; never apply breaking changes
- **Async/Await:** All reindexing operations must use async
- **Audit Logging:** Log all schema changes with structured logs
- **Idempotency:** Reindexing same documents multiple times should be safe

**Naming Conventions:**
- Modules: `snake_case` (e.g., `schema_validator.py`)
- Classes: `PascalCase` (e.g., `ReindexService`)
- Functions: `snake_case` (e.g., `validate_schema_compatibility()`)

### Testing Standards
[Source: architecture/testing-strategy.md]

**Testing Requirements:**
- **Integration Tests (services/api/tests/integration/):**
  - Test schema update with backward compatible changes
  - Test schema update rejection for breaking changes
  - Test reindexing with default field values
  - Test reindex status tracking
  - Test reindex with filters
  - Test reindex idempotency
  - Use httpx AsyncClient for API requests

**Test File Organization:**
```
services/api/tests/
├── integration/
│   └── test_schema_migration.py
└── fixtures/
    ├── old-schema.yaml
    └── new-schema.yaml
```

**Test Example:**
```python
import pytest
from httpx import AsyncClient
from fastapi import status

@pytest.mark.asyncio
async def test_schema_update_backward_compatible(
    async_client: AsyncClient,
    new_compatible_schema: dict
):
    """Test schema update with backward compatible changes."""
    response = await async_client.put(
        "/api/v1/config/metadata-schema",
        json=new_compatible_schema,
        headers={"X-API-Key": "test-key-123"}
    )

    assert response.status_code == status.HTTP_200_OK
    result = response.json()
    assert result["changes_detected"] is True
    assert result["reindex_required"] is True
    assert len(result["added_fields"]) > 0

@pytest.mark.asyncio
async def test_schema_update_breaking_change_rejected(
    async_client: AsyncClient,
    breaking_schema: dict
):
    """Test schema update rejection for breaking changes."""
    response = await async_client.put(
        "/api/v1/config/metadata-schema",
        json=breaking_schema,
        headers={"X-API-Key": "test-key-123"}
    )

    assert response.status_code == status.HTTP_400_BAD_REQUEST
    error = response.json()
    assert error["error"]["code"] == "SCHEMA_INCOMPATIBLE"
    assert "incompatibilities" in error["error"]
    assert len(error["error"]["incompatibilities"]) > 0

@pytest.mark.asyncio
async def test_reindex_applies_defaults(
    async_client: AsyncClient,
    test_documents: List[str],
    new_schema_with_defaults: dict
):
    """Test reindexing applies default values to existing documents."""
    # Update schema
    await async_client.put(
        "/api/v1/config/metadata-schema",
        json=new_schema_with_defaults,
        headers={"X-API-Key": "test-key-123"}
    )

    # Start reindex
    response = await async_client.post(
        "/api/v1/documents/reindex",
        json={},
        headers={"X-API-Key": "test-key-123"}
    )

    assert response.status_code == status.HTTP_202_ACCEPTED
    job_id = response.json()["reindex_job_id"]

    # Wait for completion
    while True:
        status_response = await async_client.get(
            f"/api/v1/documents/reindex/{job_id}/status"
        )
        status_data = status_response.json()

        if status_data["status"] == "completed":
            break

        await asyncio.sleep(1)

    # Verify documents have new field with default value
    doc_response = await async_client.get(
        f"/api/v1/documents/{test_documents[0]}",
        headers={"X-API-Key": "test-key-123"}
    )
    doc = doc_response.json()
    assert "priority" in doc["metadata"]
    assert doc["metadata"]["priority"] == 3  # Default value
```

### Previous Story Insights
[Source: docs/stories/2.2.metadata-schema.md, docs/stories/2.6.document-management.md]

**Relevant Learnings from Story 2.2:**
- Metadata schema loading and validation patterns
- Pydantic models for schema configuration
- YAML persistence for schema storage

**Relevant Learnings from Story 2.6:**
- Document repository for Neo4j operations
- Document listing and filtering
- Batch document operations

**Technical Patterns to Follow:**
- Validate schema compatibility before applying changes
- Use background tasks for long-running reindexing
- Track progress with job IDs
- Apply defaults for new optional fields
- Log all schema changes for audit trail

---

## Testing

### Test File Locations
[Source: architecture/testing-strategy.md]

**Integration Tests:**
- `services/api/tests/integration/test_schema_migration.py` - Schema migration tests

**Test Fixtures:**
- `services/api/tests/fixtures/old-schema.yaml` - Original schema
- `services/api/tests/fixtures/new-compatible-schema.yaml` - Compatible schema update
- `services/api/tests/fixtures/breaking-schema.yaml` - Incompatible schema for error testing

### Testing Approach
[Source: architecture/testing-strategy.md]

1. **Integration Tests (80%)**: Test API endpoints and reindexing workflow
2. **Unit Tests (20%)**: Test schema compatibility validation logic
3. **Coverage Target**: 80%+ for schema_validator.py and reindex_service.py

### Required Test Scenarios
- Schema update with backward compatible changes (success)
- Schema update with breaking changes (rejection)
- Reindexing with default field values
- Reindex status tracking (in_progress → completed)
- Reindex with filters (document IDs, date range)
- Reindex idempotency (multiple runs)
- Schema change audit logging
- Backward compatibility validation unit tests

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-16 | 1.0 | Story created from Epic 2 | Sarah (PO Agent) |

---

## Dev Agent Record

### Agent Model Used
*[To be populated by Dev Agent during implementation]*

### Debug Log References
*[To be populated by Dev Agent during implementation]*

### Completion Notes
*[To be populated by Dev Agent during implementation]*

### File List
*[To be populated by Dev Agent during implementation]*

---

## QA Results
*[To be populated by QA Agent after implementation]*
