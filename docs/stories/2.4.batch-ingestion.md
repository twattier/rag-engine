# Story 2.4: Implement Batch Document Ingestion and Progress Tracking

**Epic:** Epic 2 - Multi-Format Document Ingestion Pipeline
**Story ID:** 2.4
**Status:** Draft
**Estimated Effort:** 4 story points (5-6 hours)

---

## User Story

**As a** knowledge base administrator,
**I want** to upload multiple documents in a single batch operation,
**so that** I can efficiently populate large knowledge bases.

---

## Acceptance Criteria

1. API endpoint `POST /api/v1/documents/ingest/batch` accepts:
   - Multiple document files (multipart/form-data, max 100 files per batch)
   - Optional: CSV/JSON file mapping filenames to metadata
2. Batch ingestion processes documents asynchronously, returning batch_id immediately
3. Endpoint `GET /api/v1/documents/ingest/batch/{batch_id}/status` returns:
   - total_documents, processed_count, failed_count, status ("in_progress", "completed", "partial_failure")
   - List of failed documents with error messages
4. Failed document ingestion doesn't block entire batch—partial success supported
5. Background task queue (using Python asyncio or simple queue) manages batch processing with streaming upload support to prevent memory spikes (max 5GB concurrent processing limit)
6. Batch ingestion logs progress to structured logs for monitoring
7. Documentation in `docs/batch-ingestion.md` with CSV metadata format examples
8. **Performance Testing:** Integration test verifies batch ingestion of 20 documents completes in <2 minutes (>10 docs/min KPI validation)
9. Integration test verifies batch ingestion of 10 documents with mixed metadata

---

## Tasks / Subtasks

- [ ] **Task 1: Create batch ingestion endpoint** (AC: 1)
  - [ ] Add `POST /api/v1/documents/ingest/batch` to documents router
  - [ ] Accept multiple file uploads with multipart/form-data
  - [ ] Accept optional metadata mapping file (CSV or JSON)
  - [ ] Validate max 100 files per batch
  - [ ] Return 400 if batch exceeds limit
  - [ ] Generate unique batch_id (UUID)

- [ ] **Task 2: Implement metadata mapping parser** (AC: 1)
  - [ ] Create `services/api/services/metadata_mapper.py`
  - [ ] Implement CSV parser for filename-to-metadata mapping
  - [ ] Implement JSON parser for filename-to-metadata mapping
  - [ ] Handle missing files in mapping (use defaults)
  - [ ] Handle extra files not in mapping (use empty metadata)
  - [ ] Validate metadata for each file against schema

- [ ] **Task 3: Create batch processing service** (AC: 2, 4, 5)
  - [ ] Create `services/api/services/batch_service.py`
  - [ ] Implement `async def process_batch()` background task
  - [ ] Process documents sequentially to limit memory usage
  - [ ] Track processing progress (processed_count, failed_count)
  - [ ] Store batch status in memory (or Redis for future)
  - [ ] Implement error handling for individual document failures
  - [ ] Continue processing on individual failures (partial success)

- [ ] **Task 4: Implement batch status tracking** (AC: 3)
  - [ ] Create `BatchStatus` Pydantic model
  - [ ] Store batch status in in-memory dictionary (MVP)
  - [ ] Track: batch_id, total_documents, processed_count, failed_count, status
  - [ ] Track failed documents with error messages
  - [ ] Update status in real-time during processing

- [ ] **Task 5: Create batch status endpoint** (AC: 3)
  - [ ] Add `GET /api/v1/documents/ingest/batch/{batch_id}/status` to router
  - [ ] Return BatchStatusResponse with all tracking fields
  - [ ] Return 404 if batch_id not found
  - [ ] Include list of failed documents with error messages
  - [ ] Include estimated time remaining (optional enhancement)

- [ ] **Task 6: Implement memory management** (AC: 5)
  - [ ] Add streaming file upload support
  - [ ] Process files one at a time (sequential processing)
  - [ ] Add MAX_CONCURRENT_BATCH_SIZE_GB config (default 5GB)
  - [ ] Track memory usage during batch processing
  - [ ] Log warnings if approaching memory limit
  - [ ] Reject new batches if memory limit reached

- [ ] **Task 7: Add structured logging** (AC: 6)
  - [ ] Log batch start with batch_id, file count
  - [ ] Log each document processing start/completion
  - [ ] Log processing errors with document filename, error details
  - [ ] Log batch completion with summary statistics
  - [ ] Include batch_id in all log entries
  - [ ] Use structlog with JSON format

- [ ] **Task 8: Create batch ingestion documentation** (AC: 7)
  - [ ] Create `docs/batch-ingestion.md`
  - [ ] Document batch endpoint usage with examples
  - [ ] Provide CSV metadata mapping format examples
  - [ ] Provide JSON metadata mapping format examples
  - [ ] Document batch status tracking
  - [ ] Include troubleshooting section
  - [ ] Document performance considerations

- [ ] **Task 9: Create performance test** (AC: 8)
  - [ ] Create `services/api/tests/integration/test_batch_performance.py`
  - [ ] Generate 20 sample documents programmatically
  - [ ] Test batch ingestion completes in <2 minutes
  - [ ] Measure docs/min throughput
  - [ ] Log performance metrics
  - [ ] Mark as slow test (pytest marker)

- [ ] **Task 10: Create integration tests** (AC: 9)
  - [ ] Create `services/api/tests/integration/test_batch_ingestion.py`
  - [ ] Test batch ingestion with 10 documents
  - [ ] Test CSV metadata mapping
  - [ ] Test JSON metadata mapping
  - [ ] Test partial failure (some documents fail)
  - [ ] Test batch status tracking
  - [ ] Test batch limit (>100 files)
  - [ ] Test mixed file formats (PDF, DOCX, TXT)

---

## Dev Notes

### Tech Stack
[Source: architecture/tech-stack.md]

- **Python**: 3.11+ (async/await, background tasks)
- **Backend Framework**: FastAPI 0.115+ (background tasks, async endpoints)
- **Validation**: Pydantic 2.x (batch models)
- **CSV Parser**: Python csv module (metadata mapping)
- **Queue**: Python asyncio.Queue (batch processing)
- **Logging**: structlog (structured JSON logging)
- **Testing Framework**: pytest with pytest-asyncio
- **HTTP Testing**: httpx AsyncClient

### Project Structure
[Source: architecture/unified-project-structure.md]

File locations for implementation:

```
services/api/
├── routers/
│   └── documents.py             # Add batch endpoints
├── services/
│   ├── batch_service.py         # Batch processing logic
│   └── metadata_mapper.py       # CSV/JSON metadata parsing
├── models/
│   ├── requests.py              # BatchIngestRequest
│   └── responses.py             # BatchIngestResponse, BatchStatusResponse
└── tests/
    ├── integration/
    │   ├── test_batch_ingestion.py
    │   └── test_batch_performance.py
    └── fixtures/
        ├── batch-files/         # 10 sample documents
        ├── metadata-mapping.csv
        └── metadata-mapping.json

docs/
└── batch-ingestion.md           # User documentation
```

### Component Architecture
[Source: architecture/components.md]

**Batch Ingestion Responsibility:**
Process multiple documents in a single API call, track progress, handle partial failures, and manage memory efficiently.

**Key Interfaces to Implement:**
- `POST /api/v1/documents/ingest/batch` - Batch upload endpoint
- `GET /api/v1/documents/ingest/batch/{batch_id}/status` - Status tracking
- `async def process_batch(batch_id, files, metadata_mapping)` - Background processing
- `async def parse_metadata_mapping(file, format)` - CSV/JSON parsing

**Dependencies:**
- Document Service (Story 2.3)
- RAG-Anything Service (Story 2.1)
- Metadata Schema (Story 2.2)
- Neo4j Database

### API Endpoint Specification
[Source: Epic 2 Story 2.4 AC]

**Endpoint:** `POST /api/v1/documents/ingest/batch`

**Request Format:**
```
Content-Type: multipart/form-data

Fields:
- files: <multiple uploaded files> (required, max 100 files)
- metadata_mapping: <CSV or JSON file> (optional)
```

**Request Example:**
```bash
curl -X POST "http://localhost:8000/api/v1/documents/ingest/batch" \
  -H "X-API-Key: test-key-123" \
  -F "files=@doc1.pdf" \
  -F "files=@doc2.docx" \
  -F "files=@doc3.txt" \
  -F "metadata_mapping=@metadata.csv"
```

**Response Format (202 Accepted):**
```json
{
  "batch_id": "7c9e6679-7425-40de-944b-e07fc1f90ae7",
  "total_documents": 50,
  "status": "in_progress",
  "message": "Batch ingestion started. Use batch_id to check status"
}
```

**Endpoint:** `GET /api/v1/documents/ingest/batch/{batch_id}/status`

**Response Format (200 OK - In Progress):**
```json
{
  "batch_id": "7c9e6679-7425-40de-944b-e07fc1f90ae7",
  "total_documents": 50,
  "processed_count": 25,
  "failed_count": 2,
  "status": "in_progress",
  "failed_documents": [
    {
      "filename": "corrupted.pdf",
      "error": "Failed to parse PDF: File is corrupted"
    },
    {
      "filename": "invalid.txt",
      "error": "Metadata validation failed: Missing required field 'author'"
    }
  ],
  "estimated_completion_time": "2025-10-16T14:35:00Z"
}
```

**Response Format (200 OK - Completed):**
```json
{
  "batch_id": "7c9e6679-7425-40de-944b-e07fc1f90ae7",
  "total_documents": 50,
  "processed_count": 48,
  "failed_count": 2,
  "status": "completed",
  "completed_at": "2025-10-16T14:35:00Z",
  "processing_time_seconds": 120,
  "failed_documents": [
    {
      "filename": "corrupted.pdf",
      "error": "Failed to parse PDF: File is corrupted"
    },
    {
      "filename": "invalid.txt",
      "error": "Metadata validation failed: Missing required field 'author'"
    }
  ]
}
```

**Response Format (200 OK - Partial Failure):**
```json
{
  "batch_id": "7c9e6679-7425-40de-944b-e07fc1f90ae7",
  "total_documents": 50,
  "processed_count": 40,
  "failed_count": 10,
  "status": "partial_failure",
  "completed_at": "2025-10-16T14:35:00Z",
  "processing_time_seconds": 120,
  "failed_documents": [
    // List of 10 failed documents with errors
  ]
}
```

**Error Response (400 Bad Request - Too Many Files):**
```json
{
  "error": {
    "code": "BATCH_TOO_LARGE",
    "message": "Batch exceeds maximum size of 100 files. Uploaded: 150 files"
  }
}
```

### Metadata Mapping Format
[Source: Epic 2 Story 2.4 AC1]

**CSV Format:**
```csv
filename,author,department,date_created,tags,category
doc1.pdf,John Doe,Engineering,2025-10-16,"technical,api",specification
doc2.docx,Jane Smith,Marketing,2025-10-15,"marketing,campaign",report
doc3.txt,Bob Johnson,HR,2025-10-14,"hr,policy",manual
```

**JSON Format:**
```json
{
  "doc1.pdf": {
    "author": "John Doe",
    "department": "Engineering",
    "date_created": "2025-10-16",
    "tags": ["technical", "api"],
    "category": "specification"
  },
  "doc2.docx": {
    "author": "Jane Smith",
    "department": "Marketing",
    "date_created": "2025-10-15",
    "tags": ["marketing", "campaign"],
    "category": "report"
  },
  "doc3.txt": {
    "author": "Bob Johnson",
    "department": "HR",
    "date_created": "2025-10-14",
    "tags": ["hr", "policy"],
    "category": "manual"
  }
}
```

### Batch Processing Implementation
[Source: Epic 2 Story 2.4 AC2, AC4, AC5]

```python
# services/api/services/batch_service.py
from typing import Dict, List
import asyncio
from uuid import UUID, uuid4
from pydantic import BaseModel

class BatchStatus(BaseModel):
    """Batch processing status."""
    batch_id: UUID
    total_documents: int
    processed_count: int
    failed_count: int
    status: str  # "in_progress", "completed", "partial_failure"
    failed_documents: List[Dict[str, str]]
    processing_time_seconds: float = 0

class BatchService:
    """Batch document ingestion service."""

    def __init__(self, document_service):
        self.document_service = document_service
        self.batches: Dict[UUID, BatchStatus] = {}

    async def start_batch(
        self,
        files: List[UploadFile],
        metadata_mapping: Dict[str, dict]
    ) -> UUID:
        """Start batch ingestion and return batch_id."""
        batch_id = uuid4()

        # Initialize batch status
        batch_status = BatchStatus(
            batch_id=batch_id,
            total_documents=len(files),
            processed_count=0,
            failed_count=0,
            status="in_progress",
            failed_documents=[]
        )
        self.batches[batch_id] = batch_status

        # Start background processing
        asyncio.create_task(
            self._process_batch(batch_id, files, metadata_mapping)
        )

        return batch_id

    async def _process_batch(
        self,
        batch_id: UUID,
        files: List[UploadFile],
        metadata_mapping: Dict[str, dict]
    ):
        """Process batch documents sequentially."""
        batch_status = self.batches[batch_id]
        start_time = time.time()

        # Process files sequentially to limit memory usage
        for file in files:
            try:
                # Get metadata for this file
                metadata = metadata_mapping.get(file.filename, {})

                # Ingest document
                await self.document_service.ingest_document(
                    file=file,
                    metadata=metadata
                )

                batch_status.processed_count += 1

            except Exception as e:
                # Log error and continue processing
                logger.error(
                    "Batch document processing failed",
                    batch_id=str(batch_id),
                    filename=file.filename,
                    error=str(e)
                )

                batch_status.failed_count += 1
                batch_status.failed_documents.append({
                    "filename": file.filename,
                    "error": str(e)
                })

        # Update final status
        if batch_status.failed_count == 0:
            batch_status.status = "completed"
        elif batch_status.processed_count > 0:
            batch_status.status = "partial_failure"
        else:
            batch_status.status = "failed"

        batch_status.processing_time_seconds = time.time() - start_time

        logger.info(
            "Batch processing completed",
            batch_id=str(batch_id),
            total=batch_status.total_documents,
            processed=batch_status.processed_count,
            failed=batch_status.failed_count,
            duration_seconds=batch_status.processing_time_seconds
        )

    def get_batch_status(self, batch_id: UUID) -> BatchStatus:
        """Get current batch status."""
        if batch_id not in self.batches:
            raise ValueError(f"Batch {batch_id} not found")
        return self.batches[batch_id]
```

### Memory Management Strategy
[Source: Epic 2 Story 2.4 AC5]

**Memory Limits:**
- Maximum 5GB concurrent processing per batch
- Process documents sequentially (one at a time)
- Stream file uploads to disk, don't load into memory
- Clean up temporary files after processing

**Implementation:**
```python
# Track memory usage
import psutil

async def check_memory_limit():
    """Check if memory usage is within limits."""
    process = psutil.Process()
    memory_gb = process.memory_info().rss / (1024 ** 3)

    if memory_gb > MAX_CONCURRENT_BATCH_SIZE_GB:
        logger.warning(
            "Memory limit approaching",
            current_memory_gb=memory_gb,
            limit_gb=MAX_CONCURRENT_BATCH_SIZE_GB
        )
        return False
    return True
```

### Coding Standards
[Source: architecture/coding-standards.md]

**Critical Rules:**
- **Type Safety:** Use Pydantic V2 for BatchStatus model; strict type hints
- **Error Handling:** Never let individual document failure block entire batch
- **Async/Await:** All I/O operations must use async
- **Logging:** Use structlog with batch_id in all log entries
- **Memory Management:** Process files sequentially; stream uploads to disk
- **Background Tasks:** Use asyncio.create_task() for non-blocking batch processing

**Naming Conventions:**
- Modules: `snake_case` (e.g., `batch_service.py`)
- Classes: `PascalCase` (e.g., `BatchService`)
- Functions: `snake_case` (e.g., `async def process_batch()`)
- Constants: `SCREAMING_SNAKE_CASE` (e.g., `MAX_BATCH_SIZE`)

### Testing Standards
[Source: architecture/testing-strategy.md]

**Testing Requirements:**
- **Integration Tests (services/api/tests/integration/):**
  - Test batch ingestion with 10 documents
  - Test CSV metadata mapping
  - Test JSON metadata mapping
  - Test partial failure handling
  - Test batch status tracking
  - Test batch size limit (>100 files)
  - Use httpx AsyncClient for API requests
  - Use pytest-asyncio for async tests

- **Performance Tests:**
  - Test 20 documents complete in <2 minutes
  - Measure throughput (docs/min)
  - Mark as slow test with `@pytest.mark.slow`

**Test File Organization:**
```
services/api/tests/
├── integration/
│   ├── test_batch_ingestion.py     # Functional tests
│   └── test_batch_performance.py   # Performance tests
└── fixtures/
    ├── batch-files/                # 20 sample documents
    │   ├── doc1.pdf
    │   ├── doc2.docx
    │   └── ...
    ├── metadata-mapping.csv
    └── metadata-mapping.json
```

**Test Example:**
```python
import pytest
import time
from httpx import AsyncClient
from fastapi import status

@pytest.mark.asyncio
async def test_batch_ingestion_success(
    async_client: AsyncClient,
    batch_files: List[bytes],
    metadata_mapping_csv: bytes
):
    """Test successful batch ingestion with CSV metadata."""
    files = [
        ("files", (f"doc{i}.pdf", file, "application/pdf"))
        for i, file in enumerate(batch_files)
    ]
    files.append(
        ("metadata_mapping", ("metadata.csv", metadata_mapping_csv, "text/csv"))
    )

    response = await async_client.post(
        "/api/v1/documents/ingest/batch",
        files=files,
        headers={"X-API-Key": "test-key-123"}
    )

    assert response.status_code == status.HTTP_202_ACCEPTED
    result = response.json()
    assert "batch_id" in result
    assert result["total_documents"] == len(batch_files)
    assert result["status"] == "in_progress"

    # Poll status until complete
    batch_id = result["batch_id"]
    max_retries = 60
    for _ in range(max_retries):
        status_response = await async_client.get(
            f"/api/v1/documents/ingest/batch/{batch_id}/status"
        )
        status_data = status_response.json()

        if status_data["status"] in ["completed", "partial_failure"]:
            assert status_data["processed_count"] > 0
            break

        await asyncio.sleep(2)
    else:
        pytest.fail("Batch processing timeout")

@pytest.mark.slow
@pytest.mark.asyncio
async def test_batch_performance(
    async_client: AsyncClient,
    twenty_sample_files: List[bytes]
):
    """Test batch ingestion meets performance KPI (>10 docs/min)."""
    files = [
        ("files", (f"doc{i}.pdf", file, "application/pdf"))
        for i, file in enumerate(twenty_sample_files)
    ]

    start_time = time.time()

    response = await async_client.post(
        "/api/v1/documents/ingest/batch",
        files=files,
        headers={"X-API-Key": "test-key-123"}
    )

    assert response.status_code == status.HTTP_202_ACCEPTED
    batch_id = response.json()["batch_id"]

    # Wait for completion
    while True:
        status_response = await async_client.get(
            f"/api/v1/documents/ingest/batch/{batch_id}/status"
        )
        status_data = status_response.json()

        if status_data["status"] == "completed":
            break

        await asyncio.sleep(2)

    end_time = time.time()
    duration_minutes = (end_time - start_time) / 60

    # Verify performance: 20 docs in <2 minutes (>10 docs/min)
    assert duration_minutes < 2, f"Batch took {duration_minutes:.2f} minutes, expected <2 minutes"

    docs_per_minute = 20 / duration_minutes
    assert docs_per_minute >= 10, f"Throughput: {docs_per_minute:.2f} docs/min, expected >=10 docs/min"

    logger.info(
        "Batch performance test passed",
        duration_minutes=duration_minutes,
        docs_per_minute=docs_per_minute
    )
```

### Previous Story Insights
[Source: docs/stories/2.3.ingestion-api.md]

**Relevant Learnings from Story 2.3:**
- Document ingestion service already implements single document processing
- Use FastAPI background tasks for long-running operations
- Return 202 Accepted for async operations
- Use structured logging with request/operation IDs
- Handle file uploads with multipart/form-data

**Technical Patterns to Follow:**
- Reuse document_service.ingest_document() for individual files
- Use asyncio.create_task() for background processing
- Track progress in-memory (Redis upgrade in Epic 5)
- Return batch_id immediately, poll for status
- Log all batch operations for monitoring

---

## Testing

### Test File Locations
[Source: architecture/testing-strategy.md]

**Integration Tests:**
- `services/api/tests/integration/test_batch_ingestion.py` - Functional tests
- `services/api/tests/integration/test_batch_performance.py` - Performance tests

**Test Fixtures:**
- `services/api/tests/fixtures/batch-files/` - 20 sample documents for testing
- `services/api/tests/fixtures/metadata-mapping.csv` - CSV mapping example
- `services/api/tests/fixtures/metadata-mapping.json` - JSON mapping example

### Testing Approach
[Source: architecture/testing-strategy.md]

1. **Integration Tests (80%)**: Test API endpoints with real file uploads
2. **Performance Tests (20%)**: Validate KPI (>10 docs/min throughput)
3. **Coverage Target**: 80%+ for batch_service.py

### Required Test Scenarios
- Batch ingestion with 10 documents (success)
- CSV metadata mapping
- JSON metadata mapping
- Partial failure (some documents fail, others succeed)
- Batch status tracking (in_progress → completed)
- Batch size limit (>100 files rejected)
- Mixed file formats (PDF, DOCX, TXT)
- Performance test: 20 documents in <2 minutes
- Memory management (sequential processing)

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-16 | 1.0 | Story created from Epic 2 | Sarah (PO Agent) |

---

## Dev Agent Record

### Agent Model Used
*[To be populated by Dev Agent during implementation]*

### Debug Log References
*[To be populated by Dev Agent during implementation]*

### Completion Notes
*[To be populated by Dev Agent during implementation]*

### File List
*[To be populated by Dev Agent during implementation]*

---

## QA Results
*[To be populated by QA Agent after implementation]*
