# Story 3.2: Implement Entity Extraction with Custom Entity Types

**Epic:** Epic 3 - Graph-Based Retrieval, Knowledge Graph Construction & Visualization
**Story ID:** 3.2
**Status:** Draft
**Estimated Effort:** 8 story points (2-3 days)

---

## User Story

**As a** domain specialist,
**I want** LightRAG to extract domain-specific entities from my documents,
**so that** the knowledge graph reflects my specialized terminology.

---

## Acceptance Criteria

1. LightRAG entity extraction uses configured entity types from `entity-types.yaml` (Epic 2.5)
2. LLM prompt engineering includes entity type descriptions and examples for improved extraction accuracy
3. Extracted entities include: entity_type, entity_name, confidence_score, source_document_id, text_span (where entity was found)
4. Neo4j graph schema: Entity nodes with properties (name, type, embedding), Document nodes, CONTAINS relationships
5. Duplicate entity resolution: entities with similar names (fuzzy matching >90% similarity) merged into single node
6. Entity extraction logs include: document_id, entities_extracted_count, extraction_duration
7. Validation query in Neo4j confirms entities of all configured types are being created
8. Integration test with technical documentation verifies custom entity types (API, Service, Database) are extracted

---

## Tasks / Subtasks

- [ ] **Task 1: Implement entity type loading and prompt engineering** (AC: 1, 2)
  - [ ] Create `services/lightrag-integration/app/utils/entity_config.py`
  - [ ] Implement `load_entity_types(path: str) -> List[EntityType]` to parse entity-types.yaml
  - [ ] Create `build_extraction_prompt(entity_types, document_text) -> str` with entity descriptions/examples
  - [ ] Add Pydantic model `EntityType(type_name, description, examples)`
  - [ ] Cache loaded entity types in memory (reload on service restart only)

- [ ] **Task 2: Implement entity extraction with detailed metadata** (AC: 3)
  - [ ] Create `services/lightrag-integration/app/services/entity_extractor.py`
  - [ ] Implement `extract_entities(document: Dict) -> List[ExtractedEntity]`
  - [ ] Parse LLM response to extract: entity_name, entity_type, confidence_score
  - [ ] Track text_span (character offsets) where entity appears in document
  - [ ] Add source_document_id to each extracted entity
  - [ ] Return structured `ExtractedEntity` Pydantic model

- [ ] **Task 3: Design and implement Neo4j entity schema** (AC: 4)
  - [ ] Define `:Entity` node properties: `id` (UUID), `name`, `type`, `embedding` (vector), `confidence_score`, `source_doc_id`
  - [ ] Create Cypher query to create Entity nodes with vector embeddings
  - [ ] Create `(:Document)-[:CONTAINS {text_span: str}]->(:Entity)` relationships
  - [ ] Add Neo4j indexes: `CREATE INDEX entity_name_idx FOR (e:Entity) ON (e.name)`
  - [ ] Add Neo4j index: `CREATE INDEX entity_type_idx FOR (e:Entity) ON (e.type)`
  - [ ] Add vector index for entity embeddings: `CREATE VECTOR INDEX entity_embedding_idx FOR (e:Entity) ON (e.embedding)`

- [ ] **Task 4: Implement duplicate entity resolution** (AC: 5)
  - [ ] Create `services/lightrag-integration/app/services/entity_deduplication.py`
  - [ ] Implement fuzzy string matching using `fuzzywuzzy` or `rapidfuzz` library
  - [ ] For each new entity, query Neo4j for existing entities of same type with similar names (>90% similarity)
  - [ ] If duplicate found: merge entities, combine text_spans, keep highest confidence_score
  - [ ] If no duplicate: create new entity node
  - [ ] Log deduplication actions: `entity_merged` events with old/new IDs

- [ ] **Task 5: Add structured logging for entity extraction** (AC: 6)
  - [ ] Log extraction start: `entity_extraction_started` with doc_id, entity_types_count
  - [ ] Log extraction completion: `entity_extraction_completed` with doc_id, entities_extracted_count, extraction_duration_ms
  - [ ] Log entity-level details: `entity_extracted` with entity_name, entity_type, confidence_score
  - [ ] Log deduplication: `entity_deduplicated` with merged_entity_ids
  - [ ] Add performance metrics: extraction_duration_per_entity

- [ ] **Task 6: Create Neo4j validation queries** (AC: 7)
  - [ ] Create `scripts/validate-entity-extraction.py` script
  - [ ] Query: Count entities by type: `MATCH (e:Entity) RETURN e.type, count(e) ORDER BY count(e) DESC`
  - [ ] Query: Find orphan entities (not linked to documents): `MATCH (e:Entity) WHERE NOT (e)<-[:CONTAINS]-() RETURN e`
  - [ ] Query: Entity distribution per document: `MATCH (d:Document)-[:CONTAINS]->(e:Entity) RETURN d.id, count(e) AS entity_count`
  - [ ] Output validation report: entities_by_type, total_entities, orphan_entities_count

- [ ] **Task 7: Create integration tests** (AC: 8)
  - [ ] Create `services/api/tests/integration/test_entity_extraction.py`
  - [ ] Test: Upload CV document with known entities (e.g., "Python", "Google", "Senior Engineer")
  - [ ] Test: Wait for processing, query Neo4j for extracted entities
  - [ ] Test: Validate entity types match configuration (person, company, skill, technology, job)
  - [ ] Test: Check confidence scores are between 0.0-1.0
  - [ ] Test: Verify text_span populated for each entity
  - [ ] Test: Upload duplicate document, verify entity deduplication works

---

## Dev Notes

### Tech Stack
[Source: [architecture/tech-stack.md](../architecture/tech-stack.md)]

- **LightRAG**: 0.x (entity extraction engine)
- **LLM**: Ollama (local) or OpenAI via LiteLLM
- **Fuzzy Matching**: rapidfuzz (fast string similarity)
- **Neo4j**: 5.x (entity storage with vector indexes)
- **Pydantic**: 2.x (data validation)

### Entity Extraction Flow

```
Document (queued)
  → Worker fetches from queue
  → Load entity types from config/entity-types.yaml
  → Build extraction prompt with entity descriptions/examples
  → Call LLM to extract entities from document text
  → Parse LLM response → List[ExtractedEntity]
  → For each entity:
      → Check for duplicates in Neo4j (fuzzy match >90%)
      → If duplicate: merge entities
      → If new: create Entity node with embedding
      → Create (:Document)-[:CONTAINS]->(:Entity) relationship
  → Update document status to "indexed"
  → Log metrics
```

### Neo4j Schema Extensions

**New Node Type:**
```cypher
(:Entity {
  id: UUID,
  name: String,
  type: String,  // person, company, skill, etc.
  embedding: Vector,  // 384-dim for all-MiniLM-L6-v2
  confidence_score: Float,  // 0.0-1.0
  source_doc_id: UUID,
  created_at: DateTime
})
```

**New Relationship:**
```cypher
(:Document)-[:CONTAINS {
  text_span: String,  // "char 245-260" or "page 2, para 3"
  confidence: Float
}]->(:Entity)
```

**Indexes (Story 3.2):**
```cypher
CREATE INDEX entity_name_idx FOR (e:Entity) ON (e.name);
CREATE INDEX entity_type_idx FOR (e:Entity) ON (e.type);
CREATE VECTOR INDEX entity_embedding_idx FOR (e:Entity) ON (e.embedding)
  OPTIONS {indexConfig: {
    `vector.dimensions`: 384,
    `vector.similarity_function`: 'cosine'
  }};
```

### Entity Types Configuration
[Source: Epic 2.5, [config/entity-types.yaml](../../config/entity-types.yaml)]

**Available Entity Types (CV Domain):**
- person, company, domain, product, location, technology, event, document, job, skill

**Prompt Engineering Example:**
```python
def build_extraction_prompt(entity_types: List[EntityType], text: str) -> str:
    types_description = "\n".join([
        f"- {et.type_name}: {et.description}\n  Examples: {', '.join(et.examples[:3])}"
        for et in entity_types
    ])

    return f"""
Extract entities from the following document. Return a JSON list with this structure:
[{{"entity_name": "...", "entity_type": "...", "confidence": 0.0-1.0, "text_span": "..."}}]

Entity types to extract:
{types_description}

Document:
{text}

Extracted entities (JSON):
"""
```

### Duplicate Entity Resolution

**Fuzzy Matching Logic:**
```python
from rapidfuzz import fuzz

def find_duplicate_entity(
    new_entity_name: str,
    new_entity_type: str,
    neo4j_session
) -> Optional[str]:
    """Find duplicate entity in Neo4j using fuzzy matching."""
    # Query existing entities of same type
    result = neo4j_session.run("""
        MATCH (e:Entity {type: $entity_type})
        RETURN e.id AS id, e.name AS name
    """, entity_type=new_entity_type)

    for record in result:
        similarity = fuzz.ratio(new_entity_name.lower(), record["name"].lower())
        if similarity > 90:  # 90% similarity threshold
            return record["id"]

    return None
```

### Coding Standards
[Source: [architecture/coding-standards.md](../architecture/coding-standards.md)]

**Critical Rules:**
- **Type Safety**: `from __future__ import annotations` in all modules
- **Neo4j Queries**: Parameterized Cypher (prevents injection)
- **Logging**: structlog with context (doc_id, entity_type, confidence)
- **Error Handling**: Catch LLM failures, log errors, continue processing
- **Async/Await**: All DB/HTTP operations async

---

## Testing

### Test File Locations
[Source: [architecture/testing-strategy.md](../architecture/testing-strategy.md)]

**Integration Tests:**
- `services/api/tests/integration/test_entity_extraction.py`

**Unit Tests:**
- `services/lightrag-integration/tests/test_entity_extractor.py`
- `services/lightrag-integration/tests/test_entity_deduplication.py`

### Testing Approach

1. **Integration Test**: Upload CV → extract entities → validate in Neo4j
2. **Unit Test**: Mock LLM responses, test entity parsing logic
3. **Deduplication Test**: Upload duplicate document, verify entities merged
4. **Coverage Target**: 80%+ for entity extraction and deduplication code

### Test Scenarios

- **Happy Path**: CV document → entities extracted → stored in Neo4j
- **Entity Types Coverage**: Verify person, company, skill, technology extracted
- **Deduplication**: "Google" and "Google LLC" merged into single entity
- **Text Span Tracking**: Verify text_span populated with character offsets
- **Confidence Scores**: All scores between 0.0-1.0

**Test Example:**
```python
@pytest.mark.asyncio
async def test_entity_extraction_cv_document(async_client, neo4j_session):
    """Test entity extraction from CV document."""
    # Upload CV
    with open("tests/fixtures/sample-data/cv-pdfs/cv_000.pdf", "rb") as f:
        response = await async_client.post(
            "/api/v1/documents/ingest",
            files={"file": ("cv_000.pdf", f, "application/pdf")},
            data={"metadata": json.dumps({"category": "cv"})}
        )
    doc_id = response.json()["documentId"]

    # Wait for processing
    await wait_for_status(async_client, doc_id, "indexed", timeout=60)

    # Verify entities extracted
    result = neo4j_session.run("""
        MATCH (d:Document {id: $doc_id})-[:CONTAINS]->(e:Entity)
        RETURN e.type AS type, count(e) AS count
    """, doc_id=doc_id)

    entities_by_type = {rec["type"]: rec["count"] for rec in result}

    # Assertions
    assert sum(entities_by_type.values()) > 0, "No entities extracted"
    assert "person" in entities_by_type, "No person entities found"
    assert "skill" in entities_by_type, "No skill entities found"
```

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-17 | 1.0 | Story created from Epic 3 | Sarah (PO Agent) |

---

## Dev Agent Record

### Agent Model Used
<!-- Populated during implementation -->

### Debug Log References
<!-- Populated during implementation -->

### Completion Notes
<!-- Populated during implementation -->

### File List
<!-- Populated during implementation -->

---

## QA Results
<!-- Results from QA Agent review -->
