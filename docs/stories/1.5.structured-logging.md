# Story 1.5: Configure Structured Logging and Docker Compose Logging

**Epic:** Epic 1 - Foundation & Core Infrastructure
**Story ID:** 1.5
**Status:** Ready for Development
**Estimated Effort:** 2 story points (3-4 hours)

---

## User Story

**As a** developer,
**I want** structured JSON logging from all services visible via Docker Compose,
**so that** I can debug issues during development and troubleshoot production deployments.

---

## Acceptance Criteria

### AC1: API Service Structured Logging
- [ ] API service implements structured logging using `structlog`:
  - All log messages output as JSON to stdout
  - Log entries include: timestamp, level, service_name, message, context (request_id, user_id when applicable)
  - Example log entry:
    ```json
    {"timestamp": "2025-10-15T10:30:00Z", "level": "info", "service": "api", "message": "Health check requested", "request_id": "abc123"}
    ```

### AC2: Centralized Logging Configuration
- [ ] `shared/utils/logging.py` provides centralized logging configuration:
  - Configures `structlog` with consistent formatting across services
  - Log level configurable via `LOG_LEVEL` environment variable (default: INFO)
  - Development mode adds pretty-printed logs (optional, via `LOG_FORMAT=console`)

### AC3: Docker Compose Log Management
- [ ] Docker Compose configured for log management:
  - All services output logs to stdout/stderr
  - `docker-compose logs` displays logs from all services
  - `docker-compose logs -f api` follows logs for specific service

### AC4: Environment Configuration
- [ ] `.env.example` includes logging configuration (already added in Story 1.1):
  - `LOG_LEVEL` (default: INFO)
  - `LOG_FORMAT` (default: json, options: json|console)

### AC5: Logging Documentation
- [ ] Documentation in `docs/logging.md` covers:
  - Viewing logs with Docker Compose commands
  - Filtering logs by service or log level
  - Log message structure and common fields
  - Examples of typical log messages for debugging

### AC6: Key Event Logging
- [ ] API service logs key events:
  - Service startup and shutdown
  - Health check requests
  - Neo4j connection success/failure
  - Unhandled exceptions with stack traces

---

## Implementation Tasks

### Task 1: Create Centralized Logging Utility

**File:** `shared/utils/logging.py`

```python
"""
Centralized structured logging configuration for RAG Engine.

Provides consistent logging setup across all services using structlog.
"""

import logging
import sys
import structlog
from typing import Literal


LogFormat = Literal["json", "console"]


def configure_logging(
    log_level: str = "INFO",
    log_format: LogFormat = "json",
    service_name: str = "rag-engine",
):
    """
    Configure structured logging for RAG Engine services.

    Args:
        log_level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_format: Output format ("json" for production, "console" for development)
        service_name: Name of the service (for log context)

    Example:
        ```python
        from shared.utils.logging import configure_logging

        configure_logging(log_level="INFO", log_format="json", service_name="api")

        logger = structlog.get_logger(__name__)
        logger.info("service_started", port=8000)
        ```
    """
    # Configure standard library logging
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=getattr(logging, log_level.upper(), logging.INFO),
    )

    # Common processors for all log formats
    common_processors = [
        structlog.stdlib.filter_by_level,
        structlog.contextvars.merge_contextvars,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
    ]

    # Add service name to all log entries
    structlog.contextvars.clear_contextvars()
    structlog.contextvars.bind_contextvars(service=service_name)

    if log_format == "json":
        # Production: JSON logs for parsing and aggregation
        processors = common_processors + [
            structlog.processors.JSONRenderer(),
        ]
    else:
        # Development: Human-readable console logs with colors
        processors = common_processors + [
            structlog.dev.ConsoleRenderer(colors=True),
        ]

    structlog.configure(
        processors=processors,
        wrapper_class=structlog.stdlib.BoundLogger,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )


def get_logger(name: str) -> structlog.stdlib.BoundLogger:
    """
    Get a structured logger instance.

    Args:
        name: Logger name (usually __name__)

    Returns:
        Structured logger instance

    Example:
        ```python
        logger = get_logger(__name__)
        logger.info("user_login", user_id="user123", ip_address="192.168.1.1")
        ```
    """
    return structlog.get_logger(name)
```

### Task 2: Update API Service to Use Centralized Logging

**File:** `services/api/app/main.py` (update)

```python
"""
RAG Engine FastAPI application.
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager

from app.config import settings
from app.routers import health
from shared.utils.logging import configure_logging, get_logger

__version__ = "0.1.0"

logger = get_logger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle manager for application startup and shutdown."""
    # Startup: Configure logging
    configure_logging(
        log_level=settings.LOG_LEVEL,
        log_format=settings.LOG_FORMAT,
        service_name="api",
    )

    logger.info(
        "api_service_starting",
        version=__version__,
        log_level=settings.LOG_LEVEL,
        log_format=settings.LOG_FORMAT,
        neo4j_uri=settings.NEO4J_URI,
        api_port=settings.API_PORT,
    )

    yield

    # Shutdown
    logger.info("api_service_shutting_down")


# Initialize FastAPI app
app = FastAPI(
    title="RAG Engine API",
    description=(
        "Production-ready RAG API with graph-based retrieval, multi-format document processing, "
        "and multiple integration interfaces (Open-WebUI, MCP, n8n, REST)."
    ),
    version=__version__,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    lifespan=lifespan,
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(health.router, tags=["health"])


@app.get("/", tags=["root"])
async def root():
    """Root endpoint providing API information."""
    logger.debug("root_endpoint_accessed")
    return {
        "message": "RAG Engine API",
        "version": __version__,
        "docs_url": "/docs",
        "health_url": "/health",
    }


# Global exception handler for unhandled exceptions
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Log unhandled exceptions with full context."""
    logger.error(
        "unhandled_exception",
        path=request.url.path,
        method=request.method,
        error=str(exc),
        error_type=type(exc).__name__,
        exc_info=True,
    )
    return {
        "error": "internal_server_error",
        "message": "An unexpected error occurred. Please contact support.",
    }
```

### Task 3: Add Request Logging Middleware

**File:** `services/api/app/middleware.py` (new file)

```python
"""
Custom middleware for request logging and context.
"""

import time
import uuid
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware
import structlog

logger = structlog.get_logger(__name__)


class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """
    Middleware to log all HTTP requests with timing and context.
    """

    async def dispatch(self, request: Request, call_next):
        """Process request and log details."""
        # Generate unique request ID
        request_id = str(uuid.uuid4())
        structlog.contextvars.bind_contextvars(request_id=request_id)

        # Log request start
        start_time = time.time()
        logger.info(
            "request_started",
            method=request.method,
            path=request.url.path,
            client_host=request.client.host if request.client else None,
        )

        # Process request
        try:
            response = await call_next(request)

            # Log request completion
            duration_ms = (time.time() - start_time) * 1000
            logger.info(
                "request_completed",
                method=request.method,
                path=request.url.path,
                status_code=response.status_code,
                duration_ms=round(duration_ms, 2),
            )

            # Add request ID to response headers
            response.headers["X-Request-ID"] = request_id

            return response

        except Exception as e:
            # Log request failure
            duration_ms = (time.time() - start_time) * 1000
            logger.error(
                "request_failed",
                method=request.method,
                path=request.url.path,
                duration_ms=round(duration_ms, 2),
                error=str(e),
                error_type=type(e).__name__,
                exc_info=True,
            )
            raise

        finally:
            # Clear request context
            structlog.contextvars.clear_contextvars()
            structlog.contextvars.bind_contextvars(service="api")
```

**Update `services/api/app/main.py` to include middleware:**

```python
from app.middleware import RequestLoggingMiddleware

# Add after CORS middleware
app.add_middleware(RequestLoggingMiddleware)
```

### Task 4: Create Logging Documentation

**File:** `docs/logging.md`

```markdown
# Logging Guide

RAG Engine uses structured logging with `structlog` for consistent, parseable log output across all services.

## Log Format

### JSON Format (Production)

Set `LOG_FORMAT=json` in `.env` for machine-readable JSON logs:

```json
{
  "timestamp": "2025-10-15T10:30:45.123Z",
  "level": "info",
  "service": "api",
  "logger": "app.routers.health",
  "event": "health_check_requested",
  "request_id": "abc123-def456"
}
```

### Console Format (Development)

Set `LOG_FORMAT=console` in `.env` for human-readable colored logs:

```
2025-10-15 10:30:45 [info     ] health_check_requested    [app.routers.health] request_id=abc123-def456 service=api
```

## Viewing Logs

### All Services

```bash
# View all logs
docker-compose logs

# Follow all logs (live tail)
docker-compose logs -f

# View last 100 lines
docker-compose logs --tail=100
```

### Specific Service

```bash
# View API service logs
docker-compose logs api

# Follow API service logs
docker-compose logs -f api

# View Neo4j logs
docker-compose logs neo4j
```

### Filter by Timestamp

```bash
# Logs since specific time
docker-compose logs --since 2025-10-15T10:00:00

# Logs from last hour
docker-compose logs --since 1h

# Logs from last 30 minutes
docker-compose logs --since 30m
```

## Log Levels

Configure via `LOG_LEVEL` in `.env`:

- **DEBUG**: Detailed debugging information (verbose)
- **INFO**: General informational messages (default)
- **WARNING**: Warning messages for unexpected situations
- **ERROR**: Error messages for failures
- **CRITICAL**: Critical failures requiring immediate attention

### Example: Debug Mode

```bash
# .env
LOG_LEVEL=DEBUG
LOG_FORMAT=console
```

Restart services:
```bash
docker-compose restart api
```

## Common Log Events

### Service Lifecycle

```json
// Service startup
{"event": "api_service_starting", "version": "0.1.0", "level": "info"}

// Service shutdown
{"event": "api_service_shutting_down", "level": "info"}
```

### HTTP Requests

```json
// Request started
{"event": "request_started", "method": "GET", "path": "/health", "level": "info"}

// Request completed
{"event": "request_completed", "method": "GET", "path": "/health", "status_code": 200, "duration_ms": 45.2, "level": "info"}

// Request failed
{"event": "request_failed", "method": "POST", "path": "/api/v1/query", "error": "ValueError: Invalid query", "level": "error"}
```

### Neo4j Operations

```json
// Connection success
{"event": "neo4j_driver_connected", "uri": "bolt://neo4j:7687", "level": "info"}

// Connection failure
{"event": "neo4j_connection_failed", "uri": "bolt://neo4j:7687", "error": "ServiceUnavailable", "level": "error"}

// Connectivity verified
{"event": "neo4j_connectivity_verified", "response_time_ms": 42.1, "attempt": 1, "level": "info"}
```

### Health Checks

```json
// Health check failure
{"event": "health_check_neo4j_unhealthy", "error": "Connection timeout", "level": "error"}
```

### Unhandled Exceptions

```json
// Exception with stack trace
{
  "event": "unhandled_exception",
  "path": "/api/v1/query",
  "method": "POST",
  "error": "division by zero",
  "error_type": "ZeroDivisionError",
  "level": "error",
  "exc_info": "Traceback (most recent call last):\n  ..."
}
```

## Request Tracing

Each HTTP request is assigned a unique `request_id` that appears in all related log entries:

```json
{"event": "request_started", "request_id": "a1b2c3d4-e5f6-7890", ...}
{"event": "neo4j_query_executed", "request_id": "a1b2c3d4-e5f6-7890", ...}
{"event": "request_completed", "request_id": "a1b2c3d4-e5f6-7890", ...}
```

Response headers include `X-Request-ID` for client-side correlation.

## Filtering and Searching Logs

### Using `jq` (JSON Logs)

```bash
# Filter by log level
docker-compose logs api | jq 'select(.level == "error")'

# Filter by event
docker-compose logs api | jq 'select(.event == "request_completed")'

# Extract specific fields
docker-compose logs api | jq '{timestamp, level, event, duration_ms}'

# Find slow requests (> 1000ms)
docker-compose logs api | jq 'select(.duration_ms > 1000)'
```

### Using `grep` (Console Logs)

```bash
# Find errors
docker-compose logs api | grep ERROR

# Find specific event
docker-compose logs api | grep "health_check_requested"

# Find requests to specific path
docker-compose logs api | grep "path=/api/v1/query"
```

## Log Aggregation (Production)

For production deployments, consider log aggregation solutions:

### ELK Stack (Elasticsearch, Logstash, Kibana)

```yaml
# docker-compose.override.yml (production)
services:
  api:
    logging:
      driver: "syslog"
      options:
        syslog-address: "tcp://logstash:5000"
        tag: "rag-engine-api"
```

### Loki + Grafana

```yaml
services:
  api:
    logging:
      driver: "loki"
      options:
        loki-url: "http://loki:3100/loki/api/v1/push"
        labels: "service=rag-engine-api"
```

### Cloud Logging (AWS CloudWatch, GCP Cloud Logging)

Configure Docker logging driver for your cloud provider.

## Troubleshooting with Logs

### API Not Responding

```bash
# Check if API started successfully
docker-compose logs api | grep "api_service_starting"

# Look for errors during startup
docker-compose logs api | grep ERROR

# Check if port is bound
docker-compose logs api | grep "port"
```

### Neo4j Connection Issues

```bash
# Filter Neo4j-related logs
docker-compose logs api | grep neo4j

# Look for connection failures
docker-compose logs api | jq 'select(.event | contains("neo4j"))'
```

### Slow Performance

```bash
# Find slow requests (JSON format)
docker-compose logs api | jq 'select(.duration_ms > 2000)'

# Average request duration (requires jq processing)
docker-compose logs api | jq -s '[.[] | select(.duration_ms) | .duration_ms] | add / length'
```

### Recent Errors

```bash
# Last 50 error logs
docker-compose logs --tail=1000 api | jq 'select(.level == "error")' | tail -50

# Errors in last hour
docker-compose logs --since 1h api | grep ERROR
```

## Best Practices

1. **Use JSON format in production** for log aggregation and parsing
2. **Use console format in development** for readability
3. **Set appropriate log level**:
   - Production: `INFO` or `WARNING`
   - Development: `DEBUG`
   - Troubleshooting: `DEBUG` temporarily
4. **Include context** in log messages (user_id, request_id, document_id)
5. **Log errors with stack traces** using `exc_info=True`
6. **Avoid logging sensitive data** (passwords, API keys, PII)
7. **Use structured fields** instead of string interpolation

### Good Example

```python
logger.info(
    "document_ingested",
    doc_id=doc_id,
    file_name=file_name,
    file_size_bytes=file_size,
    duration_ms=duration,
)
```

### Bad Example

```python
logger.info(f"Ingested document {doc_id} with name {file_name} ({file_size} bytes) in {duration}ms")
```

## Future Enhancements

Epic 5 Story 5.3 will add:
- Metrics endpoint for Prometheus
- Performance counters and statistics
- Log sampling for high-volume production environments
- Integration with APM tools (Datadog, New Relic)
```

---

## Testing Requirements

### Unit Tests
- [ ] Test logging configuration with JSON format
- [ ] Test logging configuration with console format
- [ ] Test request middleware adds request_id
- [ ] Test exception handler logs unhandled errors

### Integration Tests
- [ ] Start API service and verify logs output to stdout
- [ ] Test log level filtering (DEBUG vs INFO)
- [ ] Verify JSON logs are valid JSON
- [ ] Test request_id appears in all related logs

### Manual Testing
- [ ] Start services with JSON format: `LOG_FORMAT=json docker-compose up api`
- [ ] Verify logs are JSON: `docker-compose logs api | tail -5`
- [ ] Start services with console format: `LOG_FORMAT=console docker-compose up api`
- [ ] Make request and verify request_id in logs
- [ ] Trigger error and verify stack trace logged

---

## Dependencies

- **Depends On:**
  - Story 1.3 (API service running)
  - Story 1.4 (health checks to log)
- **Blocks:** Story 1.6 (deployment docs reference logging)

---

## Definition of Done

- [ ] All acceptance criteria met
- [ ] Structured logging implemented with structlog
- [ ] Request logging middleware active
- [ ] Centralized logging utility created
- [ ] Documentation complete with examples
- [ ] All tests passing
- [ ] Manual testing completed
- [ ] Code committed to repository
- [ ] Story reviewed by PO (Sarah)

---

## Notes

- Structlog already initialized in Story 1.4 for Neo4j client
- This story formalizes logging across all services
- Request ID enables distributed tracing
- JSON logs optimized for log aggregation tools
- Console logs optimized for developer experience

---

**Change Log:**

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-15 | 1.0 | Story created from PRD | Sarah (PO Agent) |
