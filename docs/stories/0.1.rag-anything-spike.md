# Story 0.1: RAG-Anything Technical Validation Spike

**Epic:** Epic 0 - Pre-Epic 2 Technical Validation
**Story ID:** 0.1
**Status:** Ready for Development
**Estimated Effort:** 3 story points (2 days)

---

## User Story

**As a** technical lead,
**I want** to validate RAG-Anything library compatibility with all required document formats,
**so that** we can confidently proceed with Epic 2 or identify fallback parsers if needed.

---

## Acceptance Criteria

### AC1: Format Support Validation
- [ ] Test parsing for all 6 required formats with sample documents:
  - PDF (text-based and image-based/scanned)
  - Plain text (.txt)
  - Markdown (.md)
  - Microsoft Word (.docx)
  - Microsoft PowerPoint (.pptx)
  - CSV (.csv)
- [ ] Document which formats work correctly and which fail
- [ ] For failed formats, identify specific error messages and failure modes

### AC2: Output Structure Validation
- [ ] Document JSON output structure for each successful format
- [ ] Verify presence of expected fields: text content, metadata, extracted elements
- [ ] Test extraction of complex elements:
  - Tables (in PDF, Word, PowerPoint, and CSV)
  - Images (references or base64 in PDF, Word, PowerPoint)
  - Equations (LaTeX or MathML in PDFs)
  - Headings and structure (in Word, PowerPoint, Markdown)
  - Slide layouts (PowerPoint-specific)
  - CSV data structure preservation

### AC3: Performance Benchmarking
- [ ] Measure parsing time for documents of varying sizes:
  - Small: 1-2 pages / <100KB
  - Medium: 10-20 pages / 1-5MB
  - Large: 50+ pages / 10-50MB
- [ ] Document performance metrics in spike report
- [ ] Identify any timeout or memory issues

### AC4: Error Handling Validation
- [ ] Test parsing with malformed documents (corrupted PDF, invalid HTML)
- [ ] Document error messages and exception types
- [ ] Verify graceful degradation vs. hard failures

### AC5: Dependency Validation
- [ ] Confirm RAG-Anything and MinerU versions install successfully
- [ ] Document system dependencies (poppler-utils, tesseract-ocr, etc.)
- [ ] Test GPU acceleration (optional) vs. CPU-only mode
- [ ] Verify compatibility with Python 3.11 and Docker environment

### AC6: Fallback Parser Research
- [ ] For any failed formats, research alternative parsers:
  - PDF: pypdf, pdfplumber, pymupdf
  - Word (.docx): python-docx
  - PowerPoint (.pptx): python-pptx
  - CSV: pandas, Python csv module
  - Markdown: Python markdown library
  - Plain text: native Python file reading
- [ ] Document integration complexity and maintenance burden

### AC7: Spike Report Deliverable
- [ ] Create comprehensive spike report (`docs/architecture/rag-anything-spike-report.md`) including:
  - Executive summary with go/no-go recommendation
  - Format support matrix (6 formats × pass/fail/partial)
  - JSON output structure examples
  - Performance benchmarks table
  - Known limitations and workarounds
  - Fallback parser recommendations
  - Integration guidance for Epic 2 Story 2.1

---

## Implementation Tasks

### Task 1: Environment Setup
- [ ] Create spike workspace: `spike/rag-anything-validation/`
- [ ] Create Dockerfile for spike environment (Python 3.11 base matching project)
- [ ] Install RAG-Anything and dependencies:
  - Python: `pip install rag-anything` (verify correct package name from GitHub)
  - System: `apt-get install poppler-utils tesseract-ocr`
- [ ] Create domain-coherent sample documents (all about "Climate Change Science"):
  - Use web search to find/generate realistic content
  - PDF (text-based): Research paper excerpt with tables and equations
  - PDF (scanned): Image-based document page requiring OCR
  - TXT: Plain text research abstract
  - MD: Markdown-formatted report with headings and structure
  - DOCX: Word document with tables and images
  - PPTX: PowerPoint presentation with slide layouts and charts
  - CSV: Climate data table (temperature, CO2 levels, years)
  - Store all samples in `spike/rag-anything-validation/samples/`
  - **Rationale**: Domain coherence enables future knowledge graph visualization testing

### Task 2: Format Validation Testing
- [ ] Write Python script `test_all_formats.py` that:
  - Attempts to parse each sample document
  - Captures output JSON structure
  - Logs success/failure and error messages
  - Saves parsed output to `spike/rag-anything-validation/outputs/`
- [ ] Run script and collect results

### Task 3: Performance Benchmarking
- [ ] Create documents of varying sizes for each format
- [ ] Write benchmarking script `benchmark_parsing.py` that:
  - Measures parsing time with `time.perf_counter()`
  - Tracks memory usage (optional)
  - Outputs performance table (format × size → time)
- [ ] Run benchmarks 3 times and report average times

### Task 4: Error Handling Testing
- [ ] Create malformed sample documents (corrupted, invalid)
- [ ] **Security Note**: Run malformed document tests in isolated Docker container to prevent potential exploits from crafted files
- [ ] Test parser behavior with edge cases:
  - Corrupted PDF (truncated file)
  - Invalid Word document (wrong file extension)
  - Empty files for each format
- [ ] Document failure modes and exception types

### Task 5: Fallback Parser Research
- [ ] For any failed formats, prototype alternative parsers
- [ ] Compare output quality vs. RAG-Anything
- [ ] Document integration effort

### Task 6: Write Spike Report
- [ ] Create `docs/architecture/rag-anything-spike-report.md` with findings
- [ ] Include go/no-go recommendation for Epic 2
- [ ] Provide configuration recommendations (GPU, timeouts, fallbacks)

---

## Dev Notes

### RAG-Anything Overview
RAG-Anything is a multi-format document parsing library developed by HKUDS (Hong Kong University). It leverages **MinerU 2.0+** for high-fidelity PDF and Office document extraction.

**Key Dependencies:**
- `magic-pdf` (MinerU backend)
- `paddleocr` (optional OCR for scanned PDFs)
- `poppler-utils` (PDF rendering)
- `tesseract-ocr` (OCR engine)

**Expected Capabilities:**
- PDF: Text extraction, table detection, image extraction, equation parsing
- Office: Word (.docx) and PowerPoint (.pptx) parsing
- Markdown: Structure-aware parsing
- CSV: Tabular data extraction
- Plain text: Basic text content extraction
- Images: OCR with paddleocr for scanned PDFs

**Known Limitations (Pre-Spike):**
- Version 0.x indicates beta/pre-release status
- MinerU GPU acceleration requires CUDA setup
- Large PDF performance may vary

### Testing Standards
**Spike Methodology:**
- Use domain-coherent sample documents (Climate Change Science theme)
- Test both happy path (valid docs) and edge cases (malformed)
- Document exact library versions used
- Include sample output JSON in spike report
- **All samples share common domain** to enable future graph-based knowledge base testing with visualization

**Spike Environment:**
- Run spike in Docker container (Python 3.11 base image matching project)
- No venv required - Docker provides isolation
- Container includes all system dependencies (poppler-utils, tesseract-ocr)
- Spike workspace mounted as volume for easy access to outputs

**Success Criteria:**
- At least 5/6 formats parse successfully
- Performance acceptable (<10 seconds for 10-page PDF)
- Error messages actionable (not cryptic stack traces)
- PowerPoint slide structure preserved

**Failure Criteria:**
- Critical formats fail (PDF, DOCX, or MD)
- Performance unacceptable (>60 seconds for 10-page PDF)
- Library crashes or requires complex workarounds

---

## Dependencies

- **Depends On:** None (pre-Epic 2 spike)
- **Blocks:** Epic 2 Story 2.1 (RAG-Anything integration)

---

## Reference Documentation

- RAG-Anything GitHub: https://github.com/HKUDS/RAG-Anything
- MinerU Documentation: https://github.com/opendatalab/MinerU
- Epic 2 Requirements: [docs/stories/epic-2-ingestion.md](epic-2-ingestion.md)

---

## Definition of Done

- [ ] All 6 document formats tested (PDF, TXT, MD, DOCX, PPTX, CSV)
- [ ] Spike report created with findings
- [ ] Go/no-go recommendation documented
- [ ] Fallback parsers identified (if needed)
- [ ] Performance benchmarks recorded
- [ ] Known limitations documented
- [ ] Integration guidance provided for Story 2.1

---

## Notes

**Spike Time-Boxing:**
This spike is time-boxed to 2 days (16 hours). If RAG-Anything validation extends beyond this, document findings and recommend fallback parsers immediately.

**Spike vs. Story Difference:**
Unlike regular stories, this spike:
- Does not produce production code
- Focuses on research and validation
- Deliverable is a decision-making report, not working features
- May recommend *not* proceeding with planned approach

**Spike Outcome Scenarios:**

1. **Best Case (GO):** RAG-Anything works for 6/6 formats, performance acceptable → Proceed with Epic 2 as planned
2. **Partial Success (GO with modifications):** 4-5 formats work, 1-2 need fallback parsers → Proceed with hybrid approach
3. **Major Issues (NO-GO):** <4 formats work or critical failures → Recommend alternative: Manual integration of pypdf, python-docx, python-pptx, pandas

---

**Change Log:**

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-16 | 1.0 | Spike story created from Epic 2 pre-development review | John (PM Agent) |
| 2025-10-16 | 1.1 | Updated format requirements: Removed code parsers (.py, .js, .ts, .java, .html), focusing on document formats (PDF, TXT, MD, DOCX, PPTX, CSV) per user requirement | John (PM Agent) |
| 2025-10-16 | 1.2 | Pre-development validation improvements: Docker environment clarification (no venv needed), domain-coherent sample document strategy (Climate Change Science theme for future graph testing), security isolation note for malformed documents, explicit dependency installation commands | Sarah (PO Agent) |
