# RAG Engine Configuration Template
# Copy this file to .env and configure with your values

# ============================================================================
# CORE SERVICE CONFIGURATION
# ============================================================================

# Project Name
COMPOSE_PROJECT_NAME=rag-engine

# Environment (development, production, test)
ENVIRONMENT=development

# ============================================================================
# NEO4J CONFIGURATION
# ============================================================================

# Neo4j Authentication (username/password)
NEO4J_AUTH=neo4j/your_secure_password_here

# Neo4j connection ports (configurable to avoid conflicts)
NEO4J_HTTP_PORT=7474
NEO4J_BOLT_PORT=7687

# Neo4j Connection URIs (for internal service communication)
NEO4J_URI=bolt://neo4j:7687
NEO4J_HTTP_URI=http://neo4j:7474

# Neo4j Database Name
NEO4J_DATABASE=rag

# Neo4j Memory Configuration
# Recommended: 4GB heap for 1k documents, 8GB for larger knowledge bases
NEO4J_HEAP_INITIAL=1g
NEO4J_HEAP_MAX=2g
NEO4J_PAGECACHE=1g

# ============================================================================
# API SERVICE CONFIGURATION
# ============================================================================

# API Service Port (exposed to host)
API_PORT=8000

# API Key for Authentication (Phase 2 - leave empty for MVP)
API_KEY=

# Rate Limiting (requests per minute per API key)
RATE_LIMIT_PER_MINUTE=100

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================

# LLM Provider Options:
# - "litellm" (use LiteLLM proxy for unified access to OpenAI/Anthropic/etc)
# - "openai" (direct OpenAI API)
# - "ollama" (local Ollama instance)
# - "azure" (Azure OpenAI)
LLM_PROVIDER=ollama

# OpenAI Configuration (if using OpenAI directly or via LiteLLM)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_BASE_URL=https://api.openai.com/v1

# Anthropic Configuration (if using Anthropic via LiteLLM)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Azure OpenAI Configuration (if using Azure)
AZURE_API_KEY=
AZURE_API_BASE=
AZURE_API_VERSION=2024-02-15-preview
AZURE_DEPLOYMENT_NAME=

# Ollama Configuration (if using local Ollama)
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=qwen2.5:7b-instruct-q4_K_M

# ============================================================================
# EMBEDDING MODEL CONFIGURATION
# ============================================================================

# Embedding Provider Options:
# - "local" (sentence-transformers, no API calls)
# - "openai" (OpenAI embeddings via API)
# - "azure" (Azure OpenAI embeddings)
EMBEDDING_PROVIDER=local

# Local Embedding Model (if using sentence-transformers)
# Options: all-MiniLM-L6-v2 (384 dim), all-mpnet-base-v2 (768 dim)
LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DIM=384

# OpenAI Embedding Model (if using OpenAI)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_EMBEDDING_DIM=1536

# ============================================================================
# LITELLM PROXY CONFIGURATION (OPTIONAL)
# ============================================================================

# Enable LiteLLM Proxy Service
LITELLM_ENABLED=true

# LiteLLM Proxy Port
LITELLM_PORT=4000

# LiteLLM Internal URL (for service-to-service communication)
LITELLM_BASE_URL=http://litellm:4000

# LiteLLM Configuration File Path (mounted in Docker)
LITELLM_CONFIG_PATH=/app/litellm_config.yaml

# ============================================================================
# LIGHTRAG SERVICE CONFIGURATION
# ============================================================================

# LightRAG Working Directory (for graph storage)
LIGHTRAG_WORKING_DIR=/data/lightrag

# LightRAG Server Port (for graph visualization UI)
LIGHTRAG_SERVER_PORT=9621

# LightRAG Server Enabled (set to false to disable visualization UI)
LIGHTRAG_SERVER_ENABLED=true

# LightRAG Retrieval Configuration
LIGHTRAG_MAX_TOKENS=32768
LIGHTRAG_MAX_EMBED_TOKENS=8192

# Entity Extraction Configuration
ENTITY_TYPES_CONFIG_PATH=/app/config/entity-types.yaml

# ============================================================================
# RAG-ANYTHING SERVICE CONFIGURATION
# ============================================================================

# RAG-Anything Service Port
RAG_ANYTHING_PORT=8001

# RAG-Anything Parser Configuration
RAG_ANYTHING_PARSER=auto  # auto, ocr, txt

# MinerU Parser Configuration (for PDF/Office documents)
MINERU_ENABLED=false
MINERU_USE_GPU=false  # Set to true if GPU available for OCR

# Document Processing Limits
MAX_FILE_SIZE_MB=50
MAX_BATCH_SIZE=100

# ============================================================================
# METADATA CONFIGURATION
# ============================================================================

# Metadata Schema Configuration File
METADATA_SCHEMA_PATH=/app/config/metadata-schema.yaml

# ============================================================================
# RERANKING CONFIGURATION
# ============================================================================

# Enable Reranking (adds ~500ms latency but improves precision)
RERANKER_ENABLED=true

# Reranking Model Options:
# - "jina" (jina-reranker-v2-base-multilingual)
# - "ms-marco" (ms-marco-MiniLM-L-12-v2)
RERANKER_MODEL=jina

# Number of results to rerank (top-k from retrieval)
RERANKER_TOP_K=50

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log Level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log Format (json, console)
# Use "json" for production, "console" for development pretty-printing
LOG_FORMAT=json

# ============================================================================
# TESTING CONFIGURATION
# ============================================================================

# Preserve test database after tests (for debugging)
PRESERVE_TEST_DB=true

# ============================================================================
# BACKUP CONFIGURATION
# ============================================================================

# Backup Directory (outside Docker volumes)
BACKUP_DIR=./backups

# Automated Backup Schedule (cron format, empty to disable)
# Example: "0 2 * * *" for daily at 2 AM
BACKUP_SCHEDULE=

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Neo4j Query Timeout (seconds)
NEO4J_QUERY_TIMEOUT=30

# Retry Configuration
RETRY_MAX_ATTEMPTS=3
RETRY_BACKOFF_FACTOR=2

# Circuit Breaker Configuration
CIRCUIT_BREAKER_THRESHOLD=5
CIRCUIT_BREAKER_TIMEOUT=60

# ============================================================================
# SECURITY CONFIGURATION (PHASE 2)
# ============================================================================

# API Authentication (Phase 2)
API_AUTH_ENABLED=false

# CORS Configuration
CORS_ORIGINS=*

# ============================================================================
# MONITORING CONFIGURATION (PHASE 2)
# ============================================================================

# Prometheus Metrics Enabled
METRICS_ENABLED=false

# Metrics Port
METRICS_PORT=9090

# ============================================================================
# DOCKER NETWORK CONFIGURATION
# ============================================================================

# Docker Network Name (for service-to-service communication)
DOCKER_NETWORK=rag-engine-network

# ============================================================================
# DEPLOYMENT PLATFORM CONFIGURATION
# ============================================================================

# Platform (linux, darwin, windows)
PLATFORM=linux

# Docker Compose Profile (default, gpu, production)
COMPOSE_PROFILES=default
